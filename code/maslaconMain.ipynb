{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: ../maslaconVenv/bin/python3.9\n",
      "zsh:1: no such file or directory: ../maslaconVenv/bin/python3.9\n",
      "zsh:1: no such file or directory: ../maslaconVenv/bin/python3.9\n",
      "zsh:1: no such file or directory: ../maslaconVenv/bin/python3.9\n",
      "zsh:1: no such file or directory: ../maslaconVenv/bin/python3.9\n",
      "zsh:1: no such file or directory: ../maslaconVenv/bin/python3.9\n",
      "zsh:1: no such file or directory: ../maslaconVenv/bin/python3.9\n",
      "zsh:1: no such file or directory: ../maslaconVenv/bin/python3.9\n",
      "zsh:1: no such file or directory: ../maslaconVenv/bin/python3.9\n"
     ]
    }
   ],
   "source": [
    "!../maslaconVenv/bin/python3.9 -m pip install pandas \n",
    "!../maslaconVenv/bin/python3.9 -m pip install numpy\n",
    "!../maslaconVenv/bin/python3.9 -m pip install torch\n",
    "!../maslaconVenv/bin/python3.9 -m pip install sentence_transformers\n",
    "#!../maslaconVenv/bin/python3.9 -m pip install fasttext\n",
    "!../maslaconVenv/bin/python3.9 -m pip install sklearn\n",
    "!../maslaconVenv/bin/python3.9 -m pip install wordcloud\n",
    "!../maslaconVenv/bin/python3.9 -m pip install matplotlib\n",
    "#!../maslaconVenv/bin/python3.9 -m pip install pymorphy2\n",
    "#!../maslaconVenv/bin/python3.9 -m pip install spacy-universal-sentence-encoder\n",
    "!../maslaconVenv/bin/python3.9 -m pip install datasets\n",
    "!../maslaconVenv/bin/python3.9 -m pip install -U accelerate\n",
    "!../maslaconVenv/bin/python3.9 -m pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (24.2)\n",
      "Requirement already satisfied: transformers in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (4.45.1)\n",
      "Requirement already satisfied: filelock in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!../maslaconVenv/bin/python3.9 -m pip install --upgrade pip\n",
    "!../maslaconVenv/bin/python3.9 -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset \n",
    "#import fasttext\n",
    "#import fasttext.util\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pymorphy2\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    question = df.iloc[0, 0]  # ÐŸÐµÑ€Ð²Ñ‹Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð² Ð¿ÐµÑ€Ð²Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐµ\n",
    "    answers = df.iloc[1:, 0].tolist()  # ÐžÑ‚Ð²ÐµÑ‚Ñ‹\n",
    "    return question, answers\n",
    "\n",
    "# Ð¡Ñ‚Ñ‹Ñ€Ð¸Ð» Ð¸Ð· ÑÑ‚Ð°Ñ€Ð¾Ð¹ Ð²ÐµÑ€ÑÐ¸Ð¸\n",
    "def preprocess_text(text):\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    # ÐŸÑ€Ð¸Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ Ðº Ð½Ð¸Ð¶Ð½ÐµÐ¼Ñƒ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ñƒ\n",
    "    text = text.lower()\n",
    "    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¾Ñ‚Ñ€Ð¸Ñ†Ð°Ð½Ð¸Ð¹ Ð¸ ÑƒÑÐ¸Ð»ÐµÐ½Ð¸Ð¹\n",
    "    # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½Ð¸Ð¼ \"Ð½Ðµ\" Ñ Ð¿Ð¾ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¼ ÑÐ»Ð¾Ð²Ð¾Ð¼\n",
    "    text = re.sub(r'\\b(Ð½Ðµ|Ð½Ð¸|Ð½ÐµÑ‚)\\s+', r'\\1', text)\n",
    "    text = re.sub(r'\\b(Ð¾Ñ‡ÐµÐ½ÑŒ|ÑÐ¸Ð»ÑŒÐ½Ð¾|ÐºÑ€Ð°Ð¹Ð½Ðµ)\\s+', r'\\1', text)\n",
    "    # Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð¿ÑƒÐ½ÐºÑ‚ÑƒÐ°Ñ†Ð¸Ð¸ Ð¸ ÑÐ¿ÐµÑ†ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Ð¢Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ñ\n",
    "    words = text.split()\n",
    "    # Ð›ÐµÐ¼Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð±ÐµÐ· ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ ÑÑ‚Ð¾Ð¿-ÑÐ»Ð¾Ð²\n",
    "    words = [morph.parse(word)[0].normal_form for word in words]\n",
    "    # Ð¡Ð±Ð¾Ñ€ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾ Ð² ÑÑ‚Ñ€Ð¾ÐºÑƒ\n",
    "    return ' '.join(words)\n",
    "\n",
    "def to_lower(text):\n",
    "    text = [word.lower() for word in text]\n",
    "    return text\n",
    "\n",
    "# Ð¡Ð»ÐµÐ¿Ð¸Ð» Ð¸Ð· Ð¿ÐµÑ€Ð²Ð¾Ð¹ Ð²ÐµÑ€ÑÐ¸Ð¸\n",
    "def vectorize_tfidf(answers):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=5000)\n",
    "    embeddings = vectorizer.fit_transform(answers)\n",
    "    return embeddings\n",
    "\n",
    "# Ð’ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Sentence-BERT\n",
    "def vectorize_sentence_bert(answers):\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    embeddings = model.encode(answers)\n",
    "    return embeddings\n",
    "\n",
    "# Ð’ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð´Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ð¾Ð³Ð¾ Sentence-BERT\n",
    "def vectorize_sentence_bert_ft(answers):\n",
    "    model = SentenceTransformer('../models/sentence_bert_ft')\n",
    "    embeddings = model.encode(answers)\n",
    "    return embeddings\n",
    "\n",
    "def sbert_fine_tuning():\n",
    "    # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð±Ð°Ð·Ð¾Ð²ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…\n",
    "    df = pd.read_csv('../data/ftDataset.csv')\n",
    "    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ\n",
    "    train_examples = [InputExample(texts=[row['ÐžÑ‚Ð²ÐµÑ‚ 1'], row['ÐžÑ‚Ð²ÐµÑ‚ 2']], label=float(row['Ð¡Ñ…Ð¾Ð¶ÐµÑÑ‚ÑŒ'])) for index, row in df.iterrows()]\n",
    "    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ DataLoader Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "    # Ð—Ð°Ð´Ð°ÐµÐ¼ loss-Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ (ÐºÐ¾Ð½Ñ‚Ñ€Ð°ÑÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ)\n",
    "    train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "    # Ð”Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
    "    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=100)\n",
    "    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð´Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
    "    model.save('../models/sentence_bert_ft')\n",
    "\n",
    "# Ð’ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ FastText\n",
    "'''def vectorize_fasttext(answers):\n",
    "    fasttext.util.download_model('ru', if_exists='ignore')  # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸ FastText Ð´Ð»Ñ Ñ€ÑƒÑÑÐºÐ¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ°\n",
    "    model = fasttext.load_model('cc.ru.300.bin')\n",
    "    embeddings = [model.get_sentence_vector(answer) for answer in answers]\n",
    "    return np.array(embeddings)'''\n",
    "\n",
    "# ÐŸÐ¾Ð¸ÑÐº Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð° ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð² Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð¼ÐµÑ‚Ð¾Ð´Ð° \"Ð»Ð¾ÐºÑ‚Ñ\"\n",
    "def find_optimal_clusters_elbow(embeddings, max_clusters=10):\n",
    "    distortions = []\n",
    "    for i in range(2, max_clusters+1):\n",
    "        kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "        kmeans.fit(embeddings)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "    \n",
    "    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð° ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð² (Ñ‚Ð¾Ñ‡ÐºÐ° Ð»Ð¾ÐºÑ‚Ñ)\n",
    "    elbow_point = np.argmin(np.diff(distortions)) + 2\n",
    "    return elbow_point\n",
    "\n",
    "def find_optimal_clusters_silhouette(embeddings, max_clusters=10):\n",
    "    silhouette_scores = []\n",
    "    for i in range(2, max_clusters+1):\n",
    "        kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "        labels = kmeans.fit_predict(embeddings)\n",
    "        score = silhouette_score(embeddings, labels)\n",
    "        silhouette_scores.append(score)\n",
    "    \n",
    "    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð° ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð² (Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐ¸Ð»ÑƒÑÑ‚Ð½Ñ‹Ð¹ ÐºÐ¾ÑÑ„Ñ„Ð¸Ñ†Ð¸ÐµÐ½Ñ‚)\n",
    "    optimal_clusters = np.argmax(silhouette_scores) + 2\n",
    "    return optimal_clusters\n",
    "\n",
    "# Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð¿Ð°Ñ€Ð½Ñ‹Ñ… ÑÑ…Ð¾Ð´ÑÑ‚Ð² Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Sentence-BERT\n",
    "def calculate_similarity_sentence_bert(answers):\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    #model = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "    embeddings = model.encode(answers)\n",
    "    \n",
    "    # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð¿Ð°Ñ€Ð½Ð¾Ð³Ð¾ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð° Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ ÐºÐ¾ÑÐ¸Ð½ÑƒÑÐ½Ð¾Ð¹ Ð¼ÐµÑ€Ñ‹\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    return embeddings, similarity_matrix\n",
    "\n",
    "# ÐšÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñ‹ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð° Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð¸ÐµÑ€Ð°Ñ€Ñ…Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸\n",
    "def cluster_similarity(similarity_matrix, num_clusters):\n",
    "    clustering = AgglomerativeClustering(n_clusters=num_clusters, linkage='complete')\n",
    "    clusters = clustering.fit_predict(1 - similarity_matrix)  # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ 1 - ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð¾ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñ‹ Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ð¹\n",
    "    return clusters\n",
    "\n",
    "# ÐšÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ KMeans\n",
    "def cluster_answers(embeddings, num_clusters=3):\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=10)\n",
    "    kmeans.fit(embeddings)\n",
    "    clusters = kmeans.labels_\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Ð¡ÐžÐ—Ð”ÐÐÐ˜Ð• ÐžÐ‘Ð›ÐÐšÐžÐ’ '''\n",
    "\n",
    "# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¾Ð±Ð»Ð°ÐºÐ¾Ð² ÑÐ»Ð¾Ð² Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°\n",
    "def create_word_clouds(answers, clusters):\n",
    "    unique_clusters = np.unique(clusters)\n",
    "    summary = ''\n",
    "    \n",
    "    for cluster in unique_clusters:\n",
    "        cluster_answers = [answers[i] for i in range(len(answers)) if clusters[i] == cluster]\n",
    "        text = ' '.join(cluster_answers)\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='#c5e8e7', contour_color='#c5e8e7',\n",
    "                          font_path='./Domino/Domino Bold.otf').generate(text)\n",
    "        clusterTitle = list(wordcloud.words_.keys())[0]\n",
    "        summary += clusterTitle + ' - ' + str(len(cluster_answers)) + '\\n'\n",
    "        \n",
    "        # Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¾Ð±Ð»Ð°ÐºÐ° ÑÐ»Ð¾Ð²\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'ÐžÐ±Ð»Ð°ÐºÐ¾ \\\"{clusterTitle}\\\"')\n",
    "        plt.show()\n",
    "\n",
    "    print(summary)\n",
    "\n",
    "# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¾Ð±Ð»Ð°ÐºÐ° ÑÐ»Ð¾Ð² Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°\n",
    "def create_word_clouds_freq(answers, clusters):\n",
    "    unique_clusters = set(clusters)\n",
    "    summary = ''\n",
    "\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_answers = [answers[i] for i in range(len(answers)) if clusters[i] == cluster]\n",
    "        \n",
    "        # ÐŸÐ¾Ð´ÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñƒ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÑÐ»Ð¾Ð²Ð°\n",
    "        word_freq = Counter(cluster_answers)\n",
    "        #print(word_freq.most_common()[0][0])\n",
    "        \n",
    "        # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¾Ð±Ð»Ð°ÐºÐ° ÑÐ»Ð¾Ð²\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='#c5e8e7', contour_color='#c5e8e7',\n",
    "                          font_path='./Domino/Domino Bold.otf').generate_from_frequencies(word_freq)\n",
    "        \n",
    "        clusterTitle = list(wordcloud.words_.keys())[0]\n",
    "        summary += clusterTitle + ' - ' + str(len(cluster_answers)) + '\\n'\n",
    "\n",
    "        # Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¾Ð±Ð»Ð°ÐºÐ° ÑÐ»Ð¾Ð²\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'ÐžÐ±Ð»Ð°ÐºÐ¾ \\\"{clusterTitle}\\\"')\n",
    "        plt.show()\n",
    "\n",
    "    print(summary)\n",
    "    \n",
    "# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¾Ð±Ñ‰ÐµÐ³Ð¾ Ð¾Ð±Ð»Ð°ÐºÐ° ÑÐ»Ð¾Ð²\n",
    "def create_general_word_cloud(answers):\n",
    "    text = ' '.join(answers)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='#c5e8e7', contour_color='#c5e8e7',\n",
    "                          font_path='./Domino/Domino Bold.otf').generate(text)\n",
    "    \n",
    "    # Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¾Ð±Ð»Ð°ÐºÐ° ÑÐ»Ð¾Ð²\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('ÐžÐ±Ñ‰ÐµÐµ Ð¾Ð±Ð»Ð°ÐºÐ¾ ÑÐ»Ð¾Ð²')\n",
    "    plt.show()\n",
    "\n",
    "# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¾Ð±Ñ‰ÐµÐ³Ð¾ Ð¾Ð±Ð»Ð°ÐºÐ° ÑÐ»Ð¾Ð²\n",
    "def create_general_word_cloud_freq(answers):\n",
    "    word_freq = Counter(answers)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='#c5e8e7', contour_color='#c5e8e7',\n",
    "                          font_path='./Domino/Domino Bold.otf').generate_from_frequencies(word_freq)\n",
    "    \n",
    "    # Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¾Ð±Ð»Ð°ÐºÐ° ÑÐ»Ð¾Ð²\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('ÐžÐ±Ñ‰ÐµÐµ Ð¾Ð±Ð»Ð°ÐºÐ¾ ÑÐ»Ð¾Ð²')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Ð¡Ð›ÐžÐ’ÐÐ Ð¬ Ð¸ ÐžÐ‘Ð©Ð•Ð• ÐžÐ‘Ð›ÐÐšÐž '''\n",
    "\n",
    "# ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÑÐ°Ð¼Ð¾Ð³Ð¾ Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ð¾Ð³Ð¾ ÑÐ»Ð¾Ð²Ð° Ð² ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ðµ\n",
    "def get_dicts(answers, clusters):\n",
    "    unique_clusters = set(clusters)\n",
    "    wordsCnt = {}\n",
    "    clustersCnt = {}\n",
    "\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_answers = [answers[i] for i in range(len(answers)) if clusters[i] == cluster]\n",
    "        \n",
    "        # ÐŸÐ¾Ð´ÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñƒ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÑÐ»Ð¾Ð²Ð°\n",
    "        cnt = Counter(cluster_answers)\n",
    "        #print(cnt.most_common()[0][0])\n",
    "        wordsCnt[cnt.most_common()[0][0]] = cnt.most_common()[0][1]\n",
    "        clustersCnt[cnt.most_common()[0][0]] = len(cluster_answers)\n",
    "\n",
    "    return (wordsCnt, clustersCnt)\n",
    "\n",
    "# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¾Ð±Ñ‰ÐµÐ³Ð¾ Ð¾Ð±Ð»Ð°ÐºÐ° ÑÐ»Ð¾Ð²\n",
    "def create_cluter_word_cloud_freq(wCnt, cCnt):\n",
    "\n",
    "    for k, v in wCnt.items():\n",
    "        print(k, '-', v)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='#c5e8e7', contour_color='#c5e8e7',\n",
    "                          font_path='./Domino/Domino Bold.otf').generate_from_frequencies(wCnt)\n",
    "    \n",
    "    # Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¾Ð±Ð»Ð°ÐºÐ° ÑÐ»Ð¾Ð²\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('ÐžÐ±Ñ‰ÐµÐµ Ð¾Ð±Ð»Ð°ÐºÐ¾ ÑÐ°Ð¼Ñ‹Ñ… Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ñ… Ð¡Ð›ÐžÐ’')\n",
    "    plt.show()\n",
    "\n",
    "    for k, v in cCnt.items():\n",
    "        print(k, '-', v)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='#c5e8e7', contour_color='#c5e8e7',\n",
    "                          font_path='./Domino/Domino Bold.otf').generate_from_frequencies(cCnt)\n",
    "   \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('ÐžÐ±Ñ‰ÐµÐµ Ð¾Ð±Ð»Ð°ÐºÐ¾ ÑÐ°Ð¼Ñ‹Ñ… Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ñ… ÐšÐ›ÐÐ¡Ð¢Ð•Ð ÐžÐ’')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './reps3.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m''' MAIN '''\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m question, answers \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./reps3.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÐ’Ð¾Ð¿Ñ€Ð¾Ñ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# TF-IDF\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#answers = [preprocess_text(ans) for ans in answers]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#embTfidf = vectorize_tfidf(answers)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# BERT, Fasttext\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_path):\n\u001b[0;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     question \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# ÐŸÐµÑ€Ð²Ñ‹Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð² Ð¿ÐµÑ€Ð²Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐµ\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     answers \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# ÐžÑ‚Ð²ÐµÑ‚Ñ‹\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './reps3.csv'"
     ]
    }
   ],
   "source": [
    "''' MAIN '''\n",
    "\n",
    "\n",
    "question, answers = load_data('../data/reps3.csv')\n",
    "print(f\"Ð’Ð¾Ð¿Ñ€Ð¾Ñ: {question}\")\n",
    "\n",
    "# TF-IDF\n",
    "#answers = [preprocess_text(ans) for ans in answers]\n",
    "#embTfidf = vectorize_tfidf(answers)\n",
    "\n",
    "# BERT, Fasttext\n",
    "embBert = vectorize_sentence_bert(answers)\n",
    "#embFast = vectorize_fasttext(answers)\n",
    "\n",
    "# Similarity BERT\n",
    "#embBert, simMatr = calculate_similarity_sentence_bert(answers)\n",
    "\n",
    "#nClusters = find_optimal_clusters_silhouette(embBert)\n",
    "#nClusters = 6\n",
    "nClusters = max(7, find_optimal_clusters_silhouette(embBert, 15))\n",
    "\n",
    "# For BERT and Fasttext:\n",
    "clusters = cluster_answers(embBert, nClusters)\n",
    "\n",
    "# For Similarity BERT:\n",
    "#clusters = cluster_similarity(simMatr, nClusters)\n",
    "\n",
    "# Independent word clouds:\n",
    "create_word_clouds_freq(answers, clusters)    \n",
    "create_general_word_cloud_freq(answers)\n",
    "\n",
    "create_cluter_word_cloud_freq(*get_dicts(answers, clusters))\n",
    "\n",
    "# Ð”Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð½Ð¾Ð²Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ð’Ð¾Ð¿Ñ€Ð¾Ñ: Ð§Ñ‚Ð¾ Ð±Ñ‹ Ð²Ñ‹ Ñ…Ð¾Ñ‚ÐµÐ»Ð¸ Ð²Ð¸Ð´ÐµÑ‚ÑŒ Ð½Ð° Ñ„ÑƒÑ€ÑˆÐµÑ‚Ðµ Ð² ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ Ñ€Ð°Ð·?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nigger/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m question, answers \u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/reps3.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÐ’Ð¾Ð¿Ñ€Ð¾Ñ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43msbert_fine_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m embBert \u001b[38;5;241m=\u001b[39m vectorize_sentence_bert_ft(answers)\n\u001b[1;32m      7\u001b[0m nClusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m7\u001b[39m, find_optimal_clusters_silhouette(embBert, \u001b[38;5;241m15\u001b[39m))\n",
      "Cell \u001b[0;32mIn[23], line 60\u001b[0m, in \u001b[0;36msbert_fine_tuning\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mCosineSimilarityLoss(model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Ð”Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð´Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸\u001b[39;00m\n\u001b[1;32m     62\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/sentence_bert_ft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages/sentence_transformers/fit_mixin.py:307\u001b[0m, in \u001b[0;36mFitMixin.fit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# Transformers renamed `evaluation_strategy` to `eval_strategy` in v4.41.0\u001b[39;00m\n\u001b[1;32m    302\u001b[0m eval_strategy_key \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(transformers\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.41.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m )\n\u001b[0;32m--> 307\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformerTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_default_checkpoint_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_dataset_batch_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMultiDatasetBatchSamplers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mROUND_ROBIN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_strategy_key\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevaluation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevaluation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# load_best_model_at_end=save_best_model, # <- TODO: Look into a good solution for save_best_model\u001b[39;49;00m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_save_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_save_total_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps_per_epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m steps_per_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    329\u001b[0m     steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m([\u001b[38;5;28mlen\u001b[39m(train_dataset) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size \u001b[38;5;28;01mfor\u001b[39;00m train_dataset \u001b[38;5;129;01min\u001b[39;00m train_dataset_dict\u001b[38;5;241m.\u001b[39mvalues()])\n",
      "File \u001b[0;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, batch_sampler, multi_dataset_batch_sampler)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages/sentence_transformers/training_args.py:168\u001b[0m, in \u001b[0;36mSentenceTransformerTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__post_init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__post_init__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_sampler \u001b[38;5;241m=\u001b[39m BatchSamplers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_sampler)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_dataset_batch_sampler \u001b[38;5;241m=\u001b[39m MultiDatasetBatchSamplers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_dataset_batch_sampler)\n",
      "File \u001b[0;32m~/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages/transformers/training_args.py:1750\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1753\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1754\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torchdynamo` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_compile_backend` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1756\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   1757\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages/transformers/training_args.py:2250\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2247\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2249\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m~/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages/transformers/utils/generic.py:60\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/Documents/maslacon/maslaconVenv/lib/python3.9/site-packages/transformers/training_args.py:2123\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2123\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2124\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2125\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{ACCELERATE_MIN_VERSION}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2126\u001b[0m         )\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[1;32m   2128\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`"
     ]
    }
   ],
   "source": [
    "# ÐžÑ‚Ð´ÐµÐ»ÑŒÐ½Ð¾ Ð´Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° Ñ‚Ð¾Ð¼ Ð¶Ðµ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ðµ\n",
    "question, answers = load_data('../data/reps3.csv')\n",
    "print(f\"Ð’Ð¾Ð¿Ñ€Ð¾Ñ: {question}\")\n",
    "\n",
    "sbert_fine_tuning()\n",
    "embBert = vectorize_sentence_bert_ft(answers)\n",
    "nClusters = max(7, find_optimal_clusters_silhouette(embBert, 15))\n",
    "clusters = cluster_answers(embBert, nClusters)\n",
    "\n",
    "create_word_clouds_freq(answers, clusters)    \n",
    "create_general_word_cloud_freq(answers)\n",
    "\n",
    "create_cluter_word_cloud_freq(*get_dicts(answers, clusters))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
